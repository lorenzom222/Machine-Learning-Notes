## Gaussian Mixture

- Assumpution that are data is generated by a mixture (combination of probabilty distrubtions) of several Gaussian (or normal) distrubtions

Observed data likelihood function:
- Gaussian Mixture Distrubtion (linear superpostion of Gaussians)

$p(x)$ = sum of normal distrubutions times weight coefficient($\pi_k$)

Constraints:
- $0 \leq \pi_k \geq 1$

- sum of weight coeff for all clusters  = 1


### Properties

Gaussian component: One component is a Gaussian distrubtion that make up the mixture model. Similar to Clusters.
- Compenent is active when $z_k = 1$
- Ex. $[0,1,0]$: Here $z_2$ is active for the observing data point 
- HOWEVER, we do not have access to the one-hot vector so we need a different approach. Softer approach.


$\boldsymbol{z}$ (latent or inferred variable with $K$ space): Represent the component from which each data point is drawn
- Each data point in the dataset has its own value of z that indicates which component it was drawn from
- Ex. When $z_2 = 1$, that means for that data point it's assumed that is was generated from the 2nd component.
- Binary vector with $K$ elements, where each element can be either 0 or 1.
- 1-of-K representation, if an element ($z_k$) of $\boldsymbol{z}$ is 1 then it belongs to the k-th cluster.
- This act somewhat like a key to the ideal solution.
- We want to achieve a $\boldsymbol{z}$ what maximizes the likelihood.

$z_k$ (binary indicator): Indicate which component is active for a given data point. 
- ($z_{k} ∈ {0,1}$)
- $\sum_k z_k = 1$ or the sum of all $z_k$ elements in $\boldsymbol{z}$ is 1


$\boldsymbol{\pi}$ Mixing Vector: Contains the mixing coefficent corresponding to each cluster.
- $\boldsymbol{\pi} = [\pi_1,... ,\pi_k]$
- $0 \leq \pi_k \geq 1$
- $\sum_K \pi_k = 1$

$\pi_k$ Mixture Coefficient: Represent the prior probabilities of each component being active.
- Defines the porportion of each compenent in the mixture.
- init: Randomly or Equally chosen
- Goal: Determine the proportions of each component in the mixture
- Ex: 
	- 2 components in the mixture 
	- Mixing coefficients are 0.6 and 0.4, 
	- This means that on average, 60% of the data points are drawn from the first component and 40% of the data points are drawn from the second component.


### Graphical Model
- Marginal-distrubution: $p(\boldsymbol{z})$
	- Probability of a particular element $z_k$ being equal to 1 is given by the value of the corresponding mixing coefficient $\pi_k$
	- $p(\boldsymbol{z}) =$ product of all $\pi_k$ in $pi$ raised to the power of the corresponding element of $\boldsymbol{z}$.
$p(\boldsymbol{z}) = \prod_k \pi_k^{z_k}$

	- So if $z_2 = 1$ for some data point, $\pi_2 = p(z_2)$
	- Each data point do not exclusively belong to a certain component, but to all of them with different probability
	- Ex. if there are three components in the mixture and the mixing coefficients are $[0.2, 0.5, 0.3]$, then the probability of $z$ being equal to $[0, 1, 0]$ is $p(z=[0,1,0]) = 0.2^0 * 0.5^1 * 0.3^0 = 0.5$
	- This is a way of selecting the right value for the corresponding $z$ to $\pi$
- Conditional-distrubution: $p(\boldsymbol{x}|\boldsymbol{z})$ is equal to the product normal distrubtion raised to the power of the corresponding element of $\boldsymbol{z}$ ($z_k$). Similar to the marginal-distrubution, this is a way of selecting the corresponding distrubtions to a data point.
	- Given we have $\boldsymbol{z}$ what's the probabilty of $\boldsymbol{x}$



- Joint-distrubution: $p(\boldsymbol{x}, \boldsymbol{z}) = p(\boldsymbol{z})p(\boldsymbol{x}|\boldsymbol{z})$ 

Z ---> X


## Likelihood Function

### Sample
The marginal distribution for a *sample* being a single observation ($\boldsymbol{x}$) is the sum of the joint distrubtions over all possible states of $\boldsymbol{z}$.

$p(\boldsymbol{x}) = \sum_z p(\boldsymbol{z})p(\boldsymbol{x|z})$

We can combine the distrubtion of a particular $\boldsymbol{z}$ and the conditional distribution of $\boldsymbol{x}$ given the particular value $\boldsymbol{z}$.
- $p(\boldsymbol{z}) = \prod_{k=1} \pi_k^{z_k}$
- $p(\boldsymbol{x|z})= \prod_{k=1} \mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}\boldsymbol{\Sigma}_k)$

$p(\boldsymbol{x}) = \prod_{k=1} \pi_k^{z_k}\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}\boldsymbol{\Sigma}_k)$


### Whole dataset
$\boldsymbol{X}$: $N \cdot D$ matrix of observations
- Each row represents a single observation ($N$) from the data
- Each colunm represents a singl feature or dimension ($D$) of the data

The data is assumed to be i.i.d (“independent and identically distributed”). 
- Meaning each data point is generated by one of the components and the probabilty of this being true is the mixing coefficient ($\pi_k$)

$\boldsymbol{Z}$: $N \cdot K$ matrix of inferred variables
- Each row represents the nth point's $z$
- Each colunumns represents different hidden or inferred variables ($z{^n}_k$)

Now we can sum over the $N$ observations along with (similar to the sample) the $K$ Gaussian compenents. And we can take it's log for good measure (not 100% why we do this, but for now I think it's because it makes the math easier):

$ln p(X|π,μ,Σ) = ∑_{n=1}^{N} ln { ∑_{k=1}^{K} π_k N(x_n|μ_k,Σ_k) } = ∑_{n=1}^{N} ln { ∑_{k=1}^{K} π_k (2π)^(-D/2) |Σ_k|^(-1/2) exp(-1/2(x_n-μ_k)^T Σ_k^(-1) (x_n-μ_k)) }$


## EM Algorithm for Gaussion



## EM Algorithm

Expectation: Q($\theta$, $\theta^{old}$) = sum for all **Z** of the product of p(**Z**, **X**, $\theta^{old}$) and the natrual log of p(**Z**, **X**, $\theta$)

- p(**Z**, **X**, $\theta$): Joint distribution 
- **X**: Observed variables
- **Z**: Latent variable
- $\theta$: Governing variable

Goal: Maximize p(__X__, $\theta$) wrt $\theta$


1. init: Choose an initial setting for the param $\theta^{old}$
2. E-step: Evaluate p(**Z**, **X**, $\theta$)
3. M-step: Evaluate $\theta^{new}$ 
	- Given by:  $\theta^{new}$ argmax $Q(\theta,  \theta^{old})$
	- where Q is the expectation as stated before

- repeat 2 & 3



## Exam 2:

Understand Metal-Level EM:

- Convergence
- GMMs & HMMs

- HMMS: Know the general model
	- init: Transition Emission
	- Foward/Backwards (aka D-Step)
	- EM

## References

- Bishop Chapter 9
- https://mpatacchiola.github.io/blog/2020/07/31/gaussian-mixture-models.html
- https://web.stanford.edu/~lmackey/stats306b/doc/stats306b-spring14-lecture2_scribed.pdf
- https://machine-learning-upenn.github.io/assets/notes/Lec13_Updated.pdf






